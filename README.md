Implementaci贸n y Comparaci贸n de Modelos de Traducci贸n Autom谩tica (NMT)

Este repositorio contiene la implementaci贸n y evaluaci贸n comparativa de cuatro arquitecturas de Traducci贸n Autom谩tica Neuronal (NMT) para el par de idiomas Espa帽ol-Ingl茅s. El objetivo es analizar la evoluci贸n de las arquitecturas NMT, desde un RNN simple hasta el modelo Transformer, bajo un pipeline de datos homog茅neo.

Este proyecto fue desarrollado por estudiantes de la Universidad Andina del Cusco.

 Modelos Implementados

Se implementaron cuatro arquitecturas clave, cada una en su propio script:

simpleRNN.py

Arquitectura: Seq2Seq b谩sico con SimpleRNN.

Framework: TensorFlow (Keras).

Descripci贸n: Un modelo de l铆nea base sin mecanismo de atenci贸n.

LSTM_traductor.py

Arquitectura: Seq2Seq con BiLSTM y Atenci贸n Bahdanau.

Framework: PyTorch.

Descripci贸n: Implementa la atenci贸n aditiva cl谩sica para mejorar la captura de contexto.

traductor2.py

Arquitectura: Seq2Seq con BiGRU y Atenci贸n Bahdanau Vectorizada.

Framework: TensorFlow (Keras).

Descripci贸n: Una implementaci贸n optimizada de la atenci贸n de Bahdanau para un entrenamiento m谩s r谩pido en TensorFlow.

traductor_transformer.py

Arquitectura: Modelo Transformer completo (Encoder-Decoder).

Framework: TensorFlow (Keras).

Descripci贸n: Basado en el paper "Attention Is All You Need", utiliza 煤nicamente auto-atenci贸n y atenci贸n cruzada.

 Dataset y Preprocesamiento

Todos los modelos fueron entrenados y evaluados utilizando el mismo corpus:

Fuente: Corpus Tatoeba (ES-EN) v铆a OPUS.

Muestreo: ~50,000 pares de oraciones.

Datos Finales: ~46,104 pares (despu茅s de limpieza, normalizaci贸n y filtrado).

Tokenizaci贸n: SentencePiece (BPE). Se entrena un tokenizador sobre los datos de entrenamiento para manejar palabras raras o desconocidas (OOV) de forma efectiva.

 Resultados Comparativos

La siguiente tabla resume el rendimiento y el costo computacional de cada modelo bajo las condiciones experimentales del informe.

Modelo

Archivo

BLEU Score

Par谩metros

Tiempo (6 茅pocas)

Arq. Clave

RNN Simple

simpleRNN.py

17.40

~12.6 M

~15.0 min

SimpleRNN

LSTM

LSTM_traductor.py

25.66

~51.7 M

~28.5 min

BiLSTM + Atenci贸n

GRU

traductor2.py

40.78

~22.0 M

~116.5 min

BiGRU + Atenci贸n (Vect.)

Transformer

traductor_transformer.py

53.22

~19.7 M

~501.6 min

Multi-Head Attention

An谩lisis de Hallazgos

Calidad (BLEU): El Transformer (53.22) es el claro ganador, seguido por el GRU (40.78). Ambos modelos con atenci贸n superan significativamente a las arquitecturas m谩s antiguas.

Eficiencia (Par谩metros): El modelo LSTM (51.7 M) es el m谩s pesado, mientras que el Transformer (19.7 M) y el GRU (22.0 M) demuestran un mejor balance entre complejidad y rendimiento.

Costo (Tiempo): El Transformer (~8.4h) es, por mucho, el m谩s lento de entrenar debido a su complejidad, mientras que los modelos recurrentes son significativamente m谩s r谩pidos.

锔 Uso y Ejecuci贸n

Cada script est谩 dise帽ado para ser ejecutado de forma independiente.

Requisitos

Necesitar谩s las siguientes bibliotecas de Python:

# Para todos los modelos
pip install tensorflow sentencepiece sacrebleu

# Adicionalmente para el modelo LSTM
pip install torch


Ejecutar un Modelo

Descarga los datos del corpus Tatoeba (p.ej., Tatoeba.en-es.es y Tatoeba.en-es.en).

Coloca los archivos del corpus en el mismo directorio que los scripts.

Ejecuta el script de Python del modelo que deseas probar:


# Ejemplo para ejecutar el modelo Transformer
python traductor_transformer.py


El script se encargar谩 de todo el pipeline:

Limpiar y preprocesar los datos.

Entrenar el tokenizador SentencePiece.

Construir y entrenar el modelo.

Evaluar el BLEU score final.

Iniciar una interfaz interactiva en la consola para probar traducciones.


