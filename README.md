Implementaci√≥n y Comparaci√≥n de Modelos de Traducci√≥n Autom√°tica (NMT)

Este repositorio contiene la implementaci√≥n y evaluaci√≥n comparativa de cuatro arquitecturas de Traducci√≥n Autom√°tica Neuronal (NMT) para el par de idiomas Espa√±ol-Ingl√©s. El objetivo es analizar la evoluci√≥n de las arquitecturas NMT, desde un RNN simple hasta el modelo Transformer, bajo un pipeline de datos homog√©neo.

Este proyecto fue desarrollado por estudiantes de la Universidad Andina del Cusco.

üöÄ Modelos Implementados

Se implementaron cuatro arquitecturas clave, cada una en su propio script:

simpleRNN.py

Arquitectura: Seq2Seq b√°sico con SimpleRNN.

Framework: TensorFlow (Keras).

Descripci√≥n: Un modelo de l√≠nea base sin mecanismo de atenci√≥n.

LSTM_traductor.py

Arquitectura: Seq2Seq con BiLSTM y Atenci√≥n Bahdanau.

Framework: PyTorch.

Descripci√≥n: Implementa la atenci√≥n aditiva cl√°sica para mejorar la captura de contexto.

traductor2.py

Arquitectura: Seq2Seq con BiGRU y Atenci√≥n Bahdanau Vectorizada.

Framework: TensorFlow (Keras).

Descripci√≥n: Una implementaci√≥n optimizada de la atenci√≥n de Bahdanau para un entrenamiento m√°s r√°pido en TensorFlow.

traductor_transformer.py

Arquitectura: Modelo Transformer completo (Encoder-Decoder).

Framework: TensorFlow (Keras).

Descripci√≥n: Basado en el paper "Attention Is All You Need", utiliza √∫nicamente auto-atenci√≥n y atenci√≥n cruzada.

üìö Dataset y Preprocesamiento

Todos los modelos fueron entrenados y evaluados utilizando el mismo corpus:

Fuente: Corpus Tatoeba (ES-EN) v√≠a OPUS.

Muestreo: ~50,000 pares de oraciones.

Datos Finales: ~46,104 pares (despu√©s de limpieza, normalizaci√≥n y filtrado).

Tokenizaci√≥n: SentencePiece (BPE). Se entrena un tokenizador sobre los datos de entrenamiento para manejar palabras raras o desconocidas (OOV) de forma efectiva.

üìä Resultados Comparativos

La siguiente tabla resume el rendimiento y el costo computacional de cada modelo bajo las condiciones experimentales del informe.

Modelo

Archivo

BLEU Score

Par√°metros

Tiempo (6 √©pocas)

Arq. Clave

RNN Simple

simpleRNN.py

17.40

~12.6 M

~15.0 min

SimpleRNN

LSTM

LSTM_traductor.py

25.66

~51.7 M

~28.5 min

BiLSTM + Atenci√≥n

GRU

traductor2.py

40.78

~22.0 M

~116.5 min

BiGRU + Atenci√≥n (Vect.)

Transformer

traductor_transformer.py

53.22

~19.7 M

~501.6 min

Multi-Head Attention

An√°lisis de Hallazgos

Calidad (BLEU): El Transformer (53.22) es el claro ganador, seguido por el GRU (40.78). Ambos modelos con atenci√≥n superan significativamente a las arquitecturas m√°s antiguas.

Eficiencia (Par√°metros): El modelo LSTM (51.7 M) es el m√°s pesado, mientras que el Transformer (19.7 M) y el GRU (22.0 M) demuestran un mejor balance entre complejidad y rendimiento.

Costo (Tiempo): El Transformer (~8.4h) es, por mucho, el m√°s lento de entrenar debido a su complejidad, mientras que los modelos recurrentes son significativamente m√°s r√°pidos.

‚öôÔ∏è Uso y Ejecuci√≥n

Cada script est√° dise√±ado para ser ejecutado de forma independiente.

Requisitos

Necesitar√°s las siguientes bibliotecas de Python:

# Para todos los modelos
pip install tensorflow sentencepiece sacrebleu

# Adicionalmente para el modelo LSTM
pip install torch


Ejecutar un Modelo

Descarga los datos del corpus Tatoeba (p.ej., Tatoeba.en-es.es y Tatoeba.en-es.en).

Coloca los archivos del corpus en el mismo directorio que los scripts.

Ejecuta el script de Python del modelo que deseas probar:


# Ejemplo para ejecutar el modelo Transformer
python traductor_transformer.py


El script se encargar√° de todo el pipeline:

Limpiar y preprocesar los datos.

Entrenar el tokenizador SentencePiece.

Construir y entrenar el modelo.

Evaluar el BLEU score final.

Iniciar una interfaz interactiva en la consola para probar traducciones.

üë®‚Äçüíª Autores

Aguilar Jim√©nez, Juan Pablo

D√≠az Chura, Jhon Alexis

Espirilla Sutta, Marcelo

Villasante Garc√≠a, Julio Andr√©
